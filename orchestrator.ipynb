{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a60454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a885cfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import operator\n",
    "from typing import TypedDict, List, Annotated, Literal\n",
    "from langgraph.graph import START,END, StateGraph\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.types import Send\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d508c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafa0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override = True)\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "GROQ_API_KEY=os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7d570fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task(BaseModel):\n",
    "    id: int\n",
    "    title: str\n",
    "    \n",
    "    goal: str = Field(\n",
    "        ...,\n",
    "        description=\"One sentence describing what the reader should be able to do/understand after this section.\",\n",
    "    )\n",
    "    bullets: List[str] = Field(\n",
    "        ...,\n",
    "        min_length=3,\n",
    "        max_length=5,\n",
    "        description=\"3–5 concrete, non-overlapping subpoints to cover in this section.\",\n",
    "    )\n",
    "    target_words: int = Field(\n",
    "        ...,\n",
    "        description=\"Target word count for this section (120–450).\",\n",
    "    )\n",
    "    section_type: Literal[\n",
    "        \"intro\", \"core\", \"examples\", \"checklist\", \"common_mistakes\", \"conclusion\"\n",
    "    ] = Field(\n",
    "        ...,\n",
    "        description=\"Use 'common_mistakes' exactly once in the plan.\",\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e4d2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plan(BaseModel):\n",
    "    blog_title: str\n",
    "    audience: str = Field(..., description = 'Who is this blog for?')\n",
    "    tone: str= Field(..., description = 'Writing tone of the blog (e.g., informative, entertaining, etc.)')\n",
    "    tasks: List[Task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fcd36b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    plan: Plan\n",
    "    sections: Annotated[List[str], operator.add]\n",
    "    final: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db419e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "model = OllamaLLM(                            \n",
    "    model=\"llama3\",\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "150387bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected argument 'GOOGLE_API_KEY' provided to ChatGoogleGenerativeAI.\n"
     ]
    }
   ],
   "source": [
    "llm1 = ChatGoogleGenerativeAI(\n",
    "    model = 'gemini-2.5-flash',\n",
    "    GOOGLE_API_KEY= GOOGLE_API_KEY,\n",
    "    temperature = 0.6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatGroq(\n",
    "    groq_api_key=GROQ_API_KEY,\n",
    "    model_name='llama-3.1-70b-versatile'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d092c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelListResponse(data=[Model(id='qwen/qwen3-32b', created=1748396646, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None, max_completion_tokens=40960), Model(id='meta-llama/llama-4-scout-17b-16e-instruct', created=1743874824, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='canopylabs/orpheus-arabic-saudi', created=1765926439, object='model', owned_by='Canopy Labs', active=True, context_window=4000, public_apps=None, max_completion_tokens=50000), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='canopylabs/orpheus-v1-english', created=1766186316, object='model', owned_by='Canopy Labs', active=True, context_window=4000, public_apps=None, max_completion_tokens=50000), Model(id='groq/compound', created=1756949530, object='model', owned_by='Groq', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='allam-2-7b', created=1737672203, object='model', owned_by='SDAIA', active=True, context_window=4096, public_apps=None, max_completion_tokens=4096), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=32768), Model(id='moonshotai/kimi-k2-instruct', created=1752435491, object='model', owned_by='Moonshot AI', active=True, context_window=131072, public_apps=None, max_completion_tokens=16384), Model(id='meta-llama/llama-4-maverick-17b-128e-instruct', created=1743877158, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='groq/compound-mini', created=1756949707, object='model', owned_by='Groq', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='openai/gpt-oss-20b', created=1754407957, object='model', owned_by='OpenAI', active=True, context_window=131072, public_apps=None, max_completion_tokens=65536), Model(id='openai/gpt-oss-safeguard-20b', created=1761708789, object='model', owned_by='OpenAI', active=True, context_window=131072, public_apps=None, max_completion_tokens=65536), Model(id='meta-llama/llama-guard-4-12b', created=1746743847, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=1024), Model(id='meta-llama/llama-prompt-guard-2-22m', created=1748632101, object='model', owned_by='Meta', active=True, context_window=512, public_apps=None, max_completion_tokens=512), Model(id='openai/gpt-oss-120b', created=1754408224, object='model', owned_by='OpenAI', active=True, context_window=131072, public_apps=None, max_completion_tokens=65536), Model(id='meta-llama/llama-prompt-guard-2-86m', created=1748632165, object='model', owned_by='Meta', active=True, context_window=512, public_apps=None, max_completion_tokens=512), Model(id='moonshotai/kimi-k2-instruct-0905', created=1757046093, object='model', owned_by='Moonshot AI', active=True, context_window=262144, public_apps=None, max_completion_tokens=16384)], object='list')\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "print(client.models.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "57b93090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm2.invoke('what are multiagent systems explain in short')\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = HuggingFaceEndpoint(\n",
    "    repo_id=\"openai/gpt-oss-20b\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    ")\n",
    "llm3 = ChatHuggingFace(llm = client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d1f8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm.invoke('what are multiagent systems explain in short')\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "857f5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planner Node \n",
    "\n",
    "def orchestrator(state: State) -> dict:\n",
    "    \n",
    "    plan = llm1.with_structured_output(Plan).invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content = (\n",
    "                    \"You are a senior technical writer and developer advocate. Your job is to produce a \"\n",
    "                    \"highly actionable outline for a technical blog post.\\n\\n\"\n",
    "                    \"Hard requirements:\\n\"\n",
    "                    \"- Create 4–6 sections (tasks) that fit a technical blog.\\n\"\n",
    "                    \"- Each section must include:\\n\"\n",
    "                    \"  1) goal (1 sentence: what the reader can do/understand after the section)\\n\"\n",
    "                    \"  2) 3–5 bullets that are concrete, specific, and non-overlapping\\n\"\n",
    "                    \"  3) target word count (120–450)\\n\"\n",
    "                    \"- Include EXACTLY ONE section with section_type='common_mistakes'.\\n\\n\"\n",
    "                    \"Make it technical (not generic):\\n\"\n",
    "                    \"- Assume the reader is a developer; use correct terminology.\\n\"\n",
    "                    \"- Prefer design/engineering structure: problem → intuition → approach → implementation → \"\n",
    "                    \"trade-offs → testing/observability → conclusion.\\n\"\n",
    "                    \"- Bullets must be actionable and testable (e.g., 'Show a minimal code snippet for X', \"\n",
    "                    \"'Explain why Y fails under Z condition', 'Add a checklist for production readiness').\\n\"\n",
    "                    \"- Explicitly include at least ONE of the following somewhere in the plan (as bullets):\\n\"\n",
    "                    \"  * a minimal working example (MWE) or code sketch\\n\"\n",
    "                    \"  * edge cases / failure modes\\n\"\n",
    "                    \"  * performance/cost considerations\\n\"\n",
    "                    \"  * security/privacy considerations (if relevant)\\n\"\n",
    "                    \"  * debugging tips / observability (logs, metrics, traces)\\n\"\n",
    "                    \"- Avoid vague bullets like 'Explain X' or 'Discuss Y'. Every bullet should state what \"\n",
    "                    \"to build/compare/measure/verify.\\n\\n\"\n",
    "                    \"Ordering guidance:\\n\"\n",
    "                    \"- Start with a crisp intro and problem framing.\\n\"\n",
    "                    \"- Build core concepts before advanced details.\\n\"\n",
    "                    \"- Include one section for common mistakes and how to avoid them.\\n\"\n",
    "                    \"- End with a practical summary/checklist and next steps.\\n\\n\"\n",
    "                    \"Output must strictly match the Plan schema.\"\n",
    "                    )\n",
    "                ),\n",
    "            HumanMessage(content = f\"Topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    return {'plan': plan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68bcf827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out the no.of sections\\task that the planner Node as planned\n",
    "\n",
    "def fanout(state: State):\n",
    "    return [Send('worker',{'task':task,'topic':state['topic'],'plan': state['plan']}) for task in state['plan'].tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f309a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(payload: dict) -> dict:\n",
    "    task = payload['task']\n",
    "    topic = payload['topic']\n",
    "    plan = payload['plan']\n",
    "    \n",
    "    bullets_text = \"\\n- \"+\"\\n- \".join(task.bullets)\n",
    "    \n",
    "    section_md = model.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content = (\"You are a senior technical writer and developer advocate. Write ONE section of a technical blog post in Markdown.\\n\\n\"\n",
    "                \"Hard constraints:\\n\"\n",
    "                \"- Follow the provided Goal and cover ALL Bullets in order (do not skip or merge bullets).\\n\"\n",
    "                \"- Stay close to the Target words (±15%).\\n\"\n",
    "                \"- Output ONLY the section content in Markdown (no blog title H1, no extra commentary).\\n\\n\"\n",
    "                \"Technical quality bar:\\n\"\n",
    "                \"- Be precise and implementation-oriented (developers should be able to apply it).\\n\"\n",
    "                \"- Prefer concrete details over abstractions: APIs, data structures, protocols, and exact terms.\\n\"\n",
    "                \"- When relevant, include at least one of:\\n\"\n",
    "                \"  * a small code snippet (minimal, correct, and idiomatic)\\n\"\n",
    "                \"  * a tiny example input/output\\n\"\n",
    "                \"  * a checklist of steps\\n\"\n",
    "                \"  * a diagram described in text (e.g., 'Flow: A -> B -> C')\\n\"\n",
    "                \"- Explain trade-offs briefly (performance, cost, complexity, reliability).\\n\"\n",
    "                \"- Call out edge cases / failure modes and what to do about them.\\n\"\n",
    "                \"- If you mention a best practice, add the 'why' in one sentence.\\n\\n\"\n",
    "                \"Markdown style:\\n\"\n",
    "                \"- Start with a '## <Section Title>' heading.\\n\"\n",
    "                \"- Use short paragraphs, bullet lists where helpful, and code fences for code.\\n\"\n",
    "                \"- Avoid fluff. Avoid marketing language.\\n\"\n",
    "                \"- If you include code, keep it focused on the bullet being addressed.\\n\"\n",
    "                    )\n",
    "                ),\n",
    "            HumanMessage(\n",
    "                content = (\n",
    "                    f\"Blog: {plan.blog_title}\\n\"\n",
    "                    f\"Audience: {plan.audience}\\n\"\n",
    "                    f\"Tone: {plan.tone}\\n\"\n",
    "                    f\"Topic: {topic}\\n\\n\"\n",
    "                    f\"Section: {task.title}\\n\"\n",
    "                    f\"Section type: {task.section_type}\\n\"\n",
    "                    f\"Goal: {task.goal}\\n\"\n",
    "                    f\"Target words: {task.target_words}\\n\"\n",
    "                    f\"Bullets:{bullets_text}\\n\"\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ).strip()\n",
    "    print(section_md)\n",
    "    \n",
    "    return {'sections': [section_md]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a43df445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def reducer(state: State) -> dict:\n",
    "    \n",
    "    title = state['plan'].blog_title\n",
    "    body = \"\\n\\n\".join(state[\"sections\"]).strip()\n",
    "    \n",
    "    final_md = f\"# {title}\\n\\n{body}\\n\"\n",
    "    filename = \"\".join(c if c.isalnum() or c in (\" \", \"_\", \"-\") else \"\" for c in title)\n",
    "    filename = filename.strip().lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(filename).write_text(final_md, encoding=\"utf-8\")\n",
    "    \n",
    "    return {'final': final_md}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f52fe113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2bcd16ef9a0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = StateGraph(State)\n",
    "graph.add_node('orchestrator',orchestrator)\n",
    "graph.add_node('worker',worker)\n",
    "graph.add_node('reducer',reducer)\n",
    "\n",
    "\n",
    "graph.add_edge(START,'orchestrator')\n",
    "graph.add_conditional_edges('orchestrator',fanout,['worker'])\n",
    "graph.add_edge('worker','reducer')\n",
    "graph.add_edge('reducer',END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "24b2386b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000002BCD16EF760>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app = graph.compile()\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3940776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Beyond Single Head: The Power of Multi-Head Attention\n",
      "\n",
      "Multi-Head Attention (MHA) is a crucial component in the Transformer architecture that enables the model to capture diverse relationships between input elements. Unlike single-head attention, which focuses on a single representation subspace, MHA allows the model to jointly attend to information from different subspaces at different positions.\n",
      "\n",
      "To understand how MHA works, let's break it down into its constituent parts. In traditional self-attention mechanisms, the query (Q), key (K), and value (V) vectors are typically computed separately. In MHA, these vectors are split into multiple heads, each attending to a different representation subspace. This is achieved by performing parallel attention computations on each head, followed by concatenation and linear projection of the outputs.\n",
      "\n",
      "The benefits of MHA become apparent when considering its ability to capture varied aspects of relationships between input elements. For instance, in natural language processing (NLP), MHA can simultaneously learn syntactic and semantic dependencies between words. This is particularly useful for tasks that require modeling complex relationships, such as machine translation or text classification.\n",
      "\n",
      "In a full Transformer encoder block, MHA integrates seamlessly with other components like Layer Normalization, Feed-Forward Networks, and Residual Connections. The output of the self-attention mechanism is passed through a feed-forward network (FFN) to produce the final output. This process allows the model to capture long-range dependencies while maintaining its ability to learn diverse relationships.\n",
      "\n",
      "By using MHA in place of single-head attention, Transformer models can effectively capture complex relationships and improve their overall performance on various NLP tasks.\n",
      "## The Intuition and Mechanics of Self-Attention: Q, K, V\n",
      "\n",
      "The self-attention mechanism is at the heart of Transformer models. To understand how it works, let's start with an analogy. Imagine you're trying to summarize a long piece of text, and you want to know which parts are most important. You would naturally focus on the words that are most relevant to your summary, right? This is similar to what each word (query) does in self-attention: it looks for relevant words (keys) in the input sequence to determine their importance.\n",
      "\n",
      "The dot-product attention mechanism is used to compute raw attention scores from Query-Key pairs. This is where vector similarity comes into play. The query and key vectors are multiplied element-wise, and then the resulting vector is summed to produce a single value representing the \"similarity\" between the two inputs. This process is repeated for each input sequence element.\n",
      "\n",
      "To convert these raw scores into attention weights, we apply the softmax normalization step. This ensures that the attention weights sum to one, which is important because it allows us to treat them as probabilities. The attention weights are then used to compute a weighted average of the Value vectors, effectively creating a context vector for each token.\n",
      "\n",
      "Here's a simple example:\n",
      "```python\n",
      "query = [1, 2, 3]\n",
      "key = [4, 5, 6]\n",
      "value = [7, 8, 9]\n",
      "\n",
      "attention_scores = np.dot(query, key.T)\n",
      "attention_weights = softmax(attention_scores)\n",
      "\n",
      "context_vector = np.sum(value * attention_weights[:, None], axis=0)\n",
      "```\n",
      "In this example, the query and key vectors are multiplied element-wise to produce a set of similarity scores. The softmax function is then applied to these scores to produce attention weights that sum to one. Finally, the value vector is weighted by these attention weights to produce the context vector.\n",
      "\n",
      "By applying self-attention in this way, each token can effectively \"look\" at all other tokens in the input sequence and determine their importance. This allows the model to capture complex dependencies between different parts of the input data.\n",
      "## Mastering Self-Attention: A Foundation for Modern AI\n",
      "\n",
      "As you've learned about the intricacies of Self-Attention in this blog post, it's essential to solidify your understanding of its significance. In summary, Self-Attention enables parallel processing by allowing the model to attend to different parts of the input sequence simultaneously. This mechanism also empowers the capture of long-range dependencies, which is crucial for many NLP tasks.\n",
      "\n",
      "To put your newfound knowledge into practice or troubleshoot issues with implementing Self-Attention, follow this practical checklist:\n",
      "\n",
      "* Ensure correct QKV projection dimensions: Verify that the query, key, and value matrices are properly projected to match the attention mechanism's requirements.\n",
      "* Apply the scaling factor: Don't forget to apply the scaling factor to the attention weights to prevent numerical instability.\n",
      "* Implement robust masking logic: Develop a robust masking strategy to handle padding tokens or other special cases in your input data.\n",
      "* Proper Multi-head aggregation: Ensure that you're aggregating the outputs from multiple attention heads correctly, as this can significantly impact the model's performance.\n",
      "\n",
      "Now that you've mastered Self-Attention, it's time to take your knowledge further! Consider exploring different Transformer variants like BERT, GPT, or T5, which have been designed for specific tasks and applications. Alternatively, delve into attention visualization tools to better understand how Self-Attention is applied in practice. If you're ready for a challenge, investigate advanced attention mechanisms like Performer or Linformer, which aim to improve the efficiency and scalability of Self-Attention.\n",
      "## Avoiding Self-Attention Gotchas: Scaling, Masking, and Efficiency\n",
      "\n",
      "When implementing self-attention in Transformer models, it's easy to overlook crucial details that can lead to incorrect behavior or performance bottlenecks. Let's dive into three common mistakes and strategies to mitigate them.\n",
      "\n",
      "### **Common Mistake 1**: Incorrect or missing scaling factor (1/√d_k) in the dot-product attention\n",
      "\n",
      "The self-attention mechanism relies on a dot-product attention, which computes the weighted sum of query vectors based on their similarity to key vectors. The scaling factor `1/√d_k` is essential to prevent vanishing gradients after softmax for large `d_k`. Without this scaling, the gradients will be dominated by the magnitude of the input, leading to incorrect predictions.\n",
      "\n",
      "To avoid this mistake, ensure you include the correct scaling factor in your self-attention computation. This simple trick can make a significant difference in model performance and stability.\n",
      "\n",
      "### **Common Mistake 2**: Improper or missing attention masking\n",
      "\n",
      "Attention masking is crucial for preventing information leakage and ensuring that the model only attends to relevant tokens. Failing to apply proper masking can lead to incorrect predictions, attending to padding tokens, or even training instability.\n",
      "\n",
      "For example, consider a sequence with padding tokens at the end. If you don't mask these tokens, the model will attend to them, causing incorrect predictions. Similarly, if you're using look-ahead masks, ensure that they're correctly applied to prevent information leakage.\n",
      "\n",
      "To avoid this mistake, implement attention masking carefully, considering both padding and look-ahead scenarios. Use concrete examples or diagrams to illustrate the importance of proper masking.\n",
      "\n",
      "### **Common Mistake 3**: Overlooking quadratic complexity for long sequences\n",
      "\n",
      "Self-attention mechanisms can exhibit quadratic memory and time complexity (O(N^2)) for long sequences, leading to out-of-memory errors or excessively slow training/inference. This is particularly problematic when working with long-range dependencies or large input sequences.\n",
      "\n",
      "To alleviate this issue, consider using approximate attention mechanisms like sparse attention, local attention, or Reformer/Longformer-style approximations. These techniques can significantly reduce the computational complexity while maintaining model performance.\n",
      "## Building Self-Attention: A Minimal Python Example\n",
      "### Minimal Working Example (MWE)\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "def self_attention(Q, K, V):\n",
      "    # Linear projections for Q, K, V\n",
      "    Q = np.dot(Q, W_Q)\n",
      "    K = np.dot(K, W_K)\n",
      "    V = np.dot(V, W_V)\n",
      "\n",
      "    # Calculate attention scores\n",
      "    attention_scores = np.matmul(Q, K.T) / np.sqrt(d_k)\n",
      "\n",
      "    # Apply causal masking\n",
      "    attention_mask = np.triu(np.ones((batch_size, seq_len, seq_len)), 1)\n",
      "    attention_scores = attention_scores * attention_mask\n",
      "\n",
      "    # Compute context vectors\n",
      "    context_vectors = np.matmul(attention_scores, V)\n",
      "\n",
      "    return context_vectors\n",
      "```\n",
      "### Dimensions and Shapes\n",
      "Self-Attention operates on a batch of sequences, with each sequence having a length `seq_len` and embedding dimension `d_model`. The key takeaways are:\n",
      "\n",
      "* Q: `(batch_size, seq_len, d_model)`\n",
      "* K: `(batch_size, seq_len, d_model)`\n",
      "* V: `(batch_size, seq_len, d_model)`\n",
      "* Attention scores: `(batch_size, seq_len, seq_len)`\n",
      "* Context vectors: `(batch_size, seq_len, d_model)`\n",
      "\n",
      "### Causal Masking\n",
      "To prevent tokens from attending to future information in the sequence, we apply a causal masking mechanism. This is done by creating a mask tensor with ones for upper triangular part and zeros elsewhere:\n",
      "```python\n",
      "attention_mask = np.triu(np.ones((batch_size, seq_len, seq_len)), 1)\n",
      "```\n",
      "This ensures that attention scores are zeroed out for tokens attending to future information.\n",
      "\n",
      "### Computational Complexity\n",
      "The standard Self-Attention mechanism has a computational complexity of O(N^2 * D), where N is the sequence length and D is the embedding dimension. This can be a bottleneck for long sequences, highlighting the need for efficient attention mechanisms or parallelization techniques.\n",
      "## Introduction: Why Self-Attention Matters for Modern AI\n",
      "Self-Attention is the core mechanism behind Transformer models, revolutionizing the field of natural language processing and beyond. To understand why Self-Attention matters, let's first examine the limitations of previous sequence models.\n",
      "\n",
      "* RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory networks) suffer from the vanishing/exploding gradient problem, where gradients either shrink or blow up as they're backpropagated through time. This limits their ability to capture long-range dependencies.\n",
      "* Additionally, these models have a limited context window, making it difficult to handle sequences longer than a few hundred tokens.\n",
      "\n",
      "The Transformer architecture was introduced as a breakthrough solution to these limitations. Its parallelizability and ability to efficiently handle long-range dependencies make it an attractive alternative for sequence modeling tasks.\n",
      "\n",
      "Self-Attention is the key innovation that enables these capabilities. By allowing models to attend to different parts of the input sequence simultaneously, Self-Attention provides a powerful mechanism for capturing complex relationships between distant tokens. In this blog post, we'll dive deeper into the mechanics and benefits of Self-Attention, exploring its role in Transformer architectures and beyond.\n"
     ]
    }
   ],
   "source": [
    "out = app.invoke({'topic':'write a blog on Self Attention in Transformers','sections':[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476dd979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
