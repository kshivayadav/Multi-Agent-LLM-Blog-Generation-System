# ğŸ§  Multi-Agent LLM Blog Generation System

A production-style **AI Agent system** built using LangGraph that goes far beyond simple prompt chaining.

This project implements a **planning-based, orchestratorâ€“worker architecture** capable of:

- Planning before execution  
- Deciding when internet research is required  
- Breaking tasks into parallel subtasks  
- Using multiple worker agents  
- Adding citations and images automatically  
- Generating a complete blog end-to-end  

It demonstrates how modern AI agents are designed using structured multi-node workflows rather than single LLM calls.

---

## ğŸš€ Project Overview

Traditional LLM applications rely on:

> Prompt â†’ Model â†’ Output  

This system implements a **real agent architecture** using:

- Router
- Planner
- Worker Agents
- Reducer
- Research Tools
- Image Generation
- Structured State Management

Built using:

- **LangGraph** â€“ for multi-agent orchestration  
- **LangChain** â€“ tool & LLM integration  
- **Groq API** â€“ LLM reasoning  
- **Google Gemini API** â€“ image generation  
- **Tavily API** â€“ internet research  
- **Python** â€“ backend logic  
- **Streamlit** â€“ interactive frontend  

---

# ğŸ—ï¸ Architecture

The system follows a **Planning-Based Multi-Agent Architecture**.

User Topic
    â†“
Router Node
    â†“
Planner Node
    â†“
Parallel Worker Agents
    â†“
Research Tool (Tavily, if needed)
    â†“
Image Generator (Gemini)
    â†“
Reducer Node
    â†“
Final Blog Output


---

## ğŸ§© Core Components

### 1ï¸âƒ£ Router Node

Decides:

- Is this a simple topic?
- Does it require research?
- Should it use hybrid or open-book mode?

Routes execution path accordingly.

---

### 2ï¸âƒ£ Planner Node

Breaks the blog topic into structured sections:

- Introduction
- Technical Deep Dive
- Benchmark Analysis
- Case Studies
- Future Outlook

Creates parallel tasks for worker agents.

---

### 3ï¸âƒ£ Worker Agents

Each worker:

- Writes a specific section
- Uses research tool if required
- Adds citations
- Generates structured output

Workers operate in **parallel**, improving scalability.

---

### 4ï¸âƒ£ Research Tool Integration

Powered by **Tavily API**:

- Retrieves real-time internet information
- Filters evidence
- Extracts citations
- Supports recency filtering (e.g., last 7 days)

Enables research-powered blog generation.

---

### 5ï¸âƒ£ Image Generation Module

Uses **Gemini Image API** to:

- Generate benchmark charts
- Create architecture diagrams
- Produce energy efficiency visuals

Includes:

- Automatic retry handling
- Quota detection
- Error fallback logic

---

### 6ï¸âƒ£ Reducer Node

- Merges all worker outputs
- Formats markdown
- Cleans citations
- Assembles final blog

Produces polished output ready for publishing.

---

# ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|------------|
| Orchestration | LangGraph |
| LLM Framework | LangChain |
| LLM Provider | Groq, Ollama |
| Image Generation | Gemini |
| Research | Tavily |
| Backend | Python |
| Frontend | Streamlit |

---

# ğŸ”¬ Key Features

âœ… Planning-based execution  
âœ… Internet-aware agent  
âœ… Parallel worker agents  
âœ… Automatic citation insertion  
âœ… Automatic image generation  
âœ… Error-handling & retry logic  
âœ… Research-mode routing  
âœ… Production-style architecture  
âœ… Interview-ready project  

---

# ğŸ“‚ Project Structure


Multi-Agent-LLM-Blog-System/
â”‚
â”œâ”€â”€ Backend/
â”‚ â”œâ”€â”€ main.py # LangGraph workflow definition
â”‚ â”œâ”€â”€ prompts.py # System prompts
â”‚ â”œâ”€â”€ models.py # LLMs
â”‚ â””â”€â”€ schemas.py # State definitions
â”‚
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ app.py # Streamlit interface
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

# âš™ï¸ Installation



```bash
1ï¸âƒ£ Clone Repository

git clone https://github.com/kshivayadav/multi-agent-llm-blog-system.git
cd multi-agent-llm-blog-system

2ï¸âƒ£ Create Virtual Environment

python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install Dependencies

pip install -r requirements.txt

4ï¸âƒ£ Configure Environment Variables
Create .env file:

OPENAI_API_KEY=your_openai_key
GEMINI_API_KEY=your_gemini_key
TAVILY_API_KEY=your_tavily_key

â–¶ï¸ Run Application

streamlit run frontend/app.py
```

# ğŸ§  Agent Execution Flow (Detailed)
Step 1: User enters blog topic

Example:

The Latest LLM Releases of 2026
Step 2: Router decides mode

> Closed-book | Hybrid | Open-book


Step 3: Planner creates section plan

Example:

Context

Recent Releases

Benchmarks

Case Studies

Future Outlook

Step 4: Parallel Workers execute

Each worker:

Writes section

Uses Tavily if needed

Adds citations

Generates image

Step 5: Reducer merges outputs

Final blog generated in markdown format.

## ğŸ“Š Why This Project is Different

Most AI projects:

Use single LLM call

Rely on prompt engineering

Have no orchestration

This project demonstrates:

Graph-based workflow

Deterministic node execution

Research-aware routing

Scalable parallelism

Real-world agent architecture

## ğŸš§ Known Challenges & Learnings

Handling Gemini API quota errors (429)

Managing async parallel execution

Ensuring citation credibility

Preventing hallucinated benchmarks

State management across nodes

## ğŸ”® Future Improvements

Vector DB memory integration

Caching research results

Cost-aware routing

Model auto-selection

Observability dashboard

Human-in-the-loop review mode

## ğŸ“Œ Example Output Capabilities

The system can generate:

Research-backed AI blogs

Technical deep dives

Benchmark comparisons

Architecture explainers

Industry trend analysis

All fully formatted in Markdown.

## ğŸ“„ License

MIT License

## ğŸ¤ Contribution

Contributions are welcome.

If you'd like to extend:

Add new tools

Improve planning logic

Integrate additional LLM providers

Enhance observability

Open a PR ğŸš€

## ğŸ‘¨â€ğŸ’» Author

K Shiva Kumar

Machine Learning & GenAI Engineer

## â­ Final Thoughts

This project showcases how modern AI agents are actually built:

Not just prompts â€”
but structured planning, orchestration, tool usage, and production-grade workflows.